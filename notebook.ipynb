{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you smile! Do you want to hear another one?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "\n",
    "chain.invoke(\"Tell me a joke\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1\\nwww.wandb.ai  •  contact@wandb.ai\\nMLOps:  \\nA Holistic Approach \\nAuthors\\nDarek Kłeczek  |  Bryan Bischof  |  Hamel Husain', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 0}),\n",
       " Document(page_content='2\\nwww.wandb.ai  •  contact@wandb.ai\\nContents\\nIntroduction 3\\nMLOps — The Hard Parts 4\\nPeople\\nProcesses\\nPlatform\\nLeveling Up ML Capabilities In Your Organization 8\\nOrganization Design\\nManaging Machine Learning Projects\\nKnowledge Sharing\\nDesigning Good Processes Around ML Projects  11\\nScoping and Opportunity Sizing\\nData Discovery and Validation\\nModel Development and Evaluation\\nDevelopment Tools\\nDecision Optimization\\nGovernance\\nData Engineering\\nScaling, Orchestration, and Reproducibility\\nCI/CD and Testing For Machine Learning\\nDeployment and Observability\\nML Platforms  21\\nConsiderations When Selecting Tools\\nSkill Sets\\nAbstractions\\nExisting Infrastructure\\nPoint Solutions vs. Monoliths\\nBuild vs. Buy\\nOther Considerations\\nHow Weights & Biases Fits In\\nAddressing MLOps Opportunities With W&B  26\\nPeople\\nProcesses\\nGet In Touch 29\\nFurther Reading 29', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 1}),\n",
       " Document(page_content=\"3\\nwww.wandb.ai  •  contact@wandb.ai\\nIntroduction\\nAs machine learning (ML) capabilities continue to improve at an astounding \\nrate, the discussion around operationalizing these systems (termed MLOps) \\nis now front and center. Much of this discussion is focused on technology—\\nhowever, technology is rarely why ML projects fail.\\nML projects fail because of poor organizational structures and processes. Sometimes, a project’s \\nvalue does not justify its cost. Maybe stakeholders don’t trust the ML solution. Perhaps data isn't \\navailable at the right quantity, quality, or cost. Maybe poor software development practices slow \\ndown iteration cycles and hinder collaboration. Which is all to say that there are plenty of reasons \\nprojects don’t deliver value.\\nOur goal for this report is to describe a holistic approach to MLOps that drives business value, \\nreduces risks, and increases the success rate for ML projects. We want to provide a structure to \\ncollect best practices and to facilitate conversations around MLOps problems—and solutions. \\nSometimes, operational initiatives over-index on software, but there’s more to MLOps than just \\nyour toolbox. We’ll focus on three complementary areas:\\n• People\\nOrganizational structures and roles need to be thoughtfully designed to accommodate the \\nunique nature of ML projects. While there is some overlap between ML and software projects, \\noverlooking these differences is a costly mistake. We’ll help you avoid this.\\nAbove all else, it’s critical that people, processes, and platforms are aligned towards a shared \\ngoal: to solve a concrete business problem. In the sections that follow, we’ll share methods to \\nvalidate and maximize your ability to create business value with ML before you embark on any \\nspecific project.Effective MLOps is achieved by \\naligning people, processes, and \\nplatforms toward the shared goal of \\nsolving a concrete business problem. Solutions to practical ML \\nproblems• Processes\\nWell-designed processes ensure that ML teams have a \\npositive impact on the business by enabling organizations \\nto iterate quickly and effectively. Effective ML teams avoid \\ntaking on projects that are failures before they even start by \\nassessing often overlooked areas of operationalization, such \\nas the failure modes touched on above. We'll introduce you \\nto some best practices and offer our suggestions.\\n• Platform\\nWith principles for people and processes in place, now \\nis the right time to focus on having the right technology \\nto fully operationalize ML. We’ll discuss what you should \\nconsider when selecting tools, such as your team’s skill sets, \\nabstractions, and existing infrastructure.\", metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 2}),\n",
       " Document(page_content='4\\nwww.wandb.ai  •  contact@wandb.ai\\nMLOps — The Hard Parts\\nAlthough a majority of businesses focus on purely technological aspects of \\nMLOps, ML projects actually often stumble due to poor processes and \\norganizational structures. To illustrate why a holistic approach is needed, \\nwe’ve categorized common failure modes below.\\nConsider going through the list below and marking problems or opportunities that are relevant to \\nyour organization:\\nOrganization Design Knowledge Sharing Project ManagementPeople\\nInsufficient or \\ninfrequent iteration, \\ndue to siloes between \\ndifferent teams \\nand/or disciplines.\\nUnclear or unrealistic \\nresponsibility \\nboundaries. For \\nexample, your team \\nhas an expectation \\nthat everyone is “full \\nstack” and should \\nbe able to perform \\nplatform engineering, \\ndata science, and \\nproduct engineering \\ntasks.\\nDifferences between \\nthe metrics the \\nbusiness uses to \\nmeasure success \\nand the model \\nevaluation functions \\nor experimental \\ndesign objectives.Duplicated work and \\npoor information \\ntransfer across the \\nteam or organization.\\nOnboarding new \\npeople onto an \\nexisting project is \\nprohibitively onerous \\nand time-consuming \\nrelative to other \\nengineering projects.\\nPoor internal \\ndocumentation \\npractices that reduce \\ncollaboration, \\nincrease siloing, \\nand lead to different \\nteams reinventing the \\nsame tools.Goals and incentives \\ndo not allow data \\nscientists to conduct \\nexperiments \\nwhere outcomes \\nare unknown a \\npriori. Instead, the \\nwork is focused on \\npredictable, low-risk—\\nand thus often low-\\nROI—hypotheses.\\nManagement and \\ndirection of data \\nscience tasks do \\nnot properly reflect \\nthe R&D nature of \\nmachine learning.\\nUsage of software \\ndevelopment-\\ncentric management \\ntechniques (such \\nas agile, scrum, or \\nwaterfall), despite \\ntheir ineffectiveness \\nin managing ML \\nprojects.1\\n1. \"Why Agile is bad for ML ” Forbes', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 3}),\n",
       " Document(page_content='5\\nwww.wandb.ai  •  contact@wandb.ai\\nScoping and  \\nOpportunity SizingModel Development  \\nand EvaluationData Discovery  \\nand Validation\\nGovernanceProcesses\\nApproved projects \\nare not suitable for \\nmachine learning, or \\nthe problems have not \\nappropriately been \\nframed as machine \\nlearning tasks.\\nTheoretical value of \\napproved projects \\ndoes not justify the \\ncost of development.\\nStakeholders don’t \\ntrust or adopt the \\nmachine learning \\nsolutions that have \\nbeen delivered.Equipping developers \\nwith the right \\nenvironment (ML-\\nspecific hardware \\nand dependencies) \\nand/or maintaining \\nconsistency \\nbetween developer \\nenvironments is \\nchallenging.\\nPoor software \\ndevelopment \\npractices slow down \\niteration speed. For \\nexample, spending \\ntoo much time \\nrefactoring code from \\nnotebooks to scripts \\nand modules.\\nManual processes \\n(e.g. manual \\nexperiment tracking) \\nthat lead to unreliable \\nand irreproducible \\nresults, or results \\nthat are hard to share \\namongst the team.\\nGetting ML engineers \\nstarted with the \\ncompute and tooling \\nthey need is difficult.\\nMigrating model \\nprototypes to \\nproduction requires \\nsignificant effort, on \\nthe scale of months \\nto years.Data that is relevant \\nto training the models \\nthat solve business \\nproblems does \\nnot exist.\\nRelevant data that \\ndoes exist is noisy \\nand low quality.\\nImproving data \\nlabeling is prohibitively \\nexpensive.\\nEnsuring that the data \\nused to train or test \\nmodels complies with \\ndata governance \\nrestrictions is difficult.\\nAuditing undesirable \\nmodel biases that \\nmay affect your \\ncompany brand is \\nchallenging.\\nExternal regulation \\nrequirements such \\nas validation or \\ndocumentation are \\nnot explicit priorities.Complex and/or \\ncomputationally \\nexpensive feature \\nengineering \\nrequires expensive \\narchitectural \\nexceptions in your \\ninfrastructure.\\nNo support for model \\noperationalization \\nwith existing data \\norchestration tools.\\nWorking with online \\nfeatures requires \\nsignificant effort \\nand is not easily \\ncombined with offline \\nfeature stores.Data Engineering', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 4}),\n",
       " Document(page_content='6\\nwww.wandb.ai  •  contact@wandb.ai\\nCI/CD and Testing  \\nFor Machine LearningScaling, Orchestration,  \\nand Reproducibility\\nSoftware that is \\nwritten to test \\nworkflows does not \\nfollow best practices \\n(unit tests, integration \\ntests, etc.)\\nManual and \\ninconsistent tests.\\nManual copy and \\npasting of information \\nfrom other platforms \\ninto code reviews  \\n(e.g. screenshots).Scaling from single \\nto multiple models \\n(e.g. one model for \\neach customer, daily \\nretraining, ensembles, \\nshadow deployments) \\nis not supported by the \\nexisting infrastructure \\nor requires additional \\ncapabilities that the \\nteam realizes late in \\nthe project lifecycle.\\nNo good process \\nor mechanism for \\nautomated testing\\nEvaluation of ML \\nmodels is not \\nstraightforward.\\nAdding feedback \\nloops from production \\ncauses unforeseen side \\neffects.\\nChanges in the external \\nenvironment and \\nproduction data \\nfrequently degrade \\nmodel performance.\\nPlanning for scale \\nis often overlooked, \\nleading to high \\nmaintenance costs, \\npoor user experience, \\nand even cascading \\nfailures.\\nConstant re-\\narchitecture of the tech \\nstack as the solution \\nscales.\\nA machine learning \\nplatform that is always \\nlagging behind the \\nneeds of the product.Deployment and \\nObservability\\nModel inference is \\nunable to scale to \\nmeet production \\nvolumes.\\nProduction models \\ndo not perform well \\nbecause the data \\nin production is \\ninconsistent with data \\nduring training.Decision \\nOptimization\\nMonitoring of \\ntechnical metrics that \\ndo not reconcile with \\nthe business metrics \\nused for decision-\\nmaking.\\nUsage of models \\nwithout calibrating \\nperformance to \\ndecision-making \\ncriteria that could \\nmaximize \\nbusiness impact.\\nFailure to articulate \\nhow a model will \\nimprove business \\noutcomes in a way \\nthat resonates with \\nstakeholders.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 5}),\n",
       " Document(page_content='7\\nwww.wandb.ai  •  contact@wandb.ai\\nThe checklists above should help you narrow down your core areas of focus. How you prioritize \\nthese problems and categories depends on the specific details of your business, such as your scale, \\nmaturity, and technical capabilities. However, these risks are connected, and their nuance and \\nvariety necessitate a holistic approach to MLOps.\\nIn the next section, we’ll unpack these three categories—people, processes, and platforms—in \\nmore detail, and provide you with tools that you can apply in your own organization.Fit for Purpose Maintainability, \\nInteroperability, and \\nReliabilityPlatform\\nHiring skilled \\nengineers to build \\ncustom software is a \\ncommon bottleneck \\nto project completion.\\nInternal tool \\ndevelopment is \\nsuboptimal or \\noverruns \\nyour budget.\\nCustom software \\ndevelopment delays \\nrealization of \\nbusiness value.Employees seem \\nunsatisfied with \\ntheir tools and \\ncomplain about \\nnegative impacts on \\nproductivity.\\nTools require team \\nmembers to spend \\ntoo much time \\ndeveloping skill sets \\nthat are orthogonal to \\ntheir function.Software maintenance \\nis at risk due to \\ndeveloper churn.  \\nMonitoring, \\ntroubleshooting, \\nand improving \\ndeveloper experience \\nrequire specialized \\ndevelopment. \\nIntegrating with \\nvarious ML tools \\nconsumes significant \\ndevelopment \\nbandwidth. \\nHardware or \\nsoftware failures are \\ndisruptive to business \\noperations. \\nML systems fail silently.Cost and Time to \\nValue', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 6}),\n",
       " Document(page_content='8\\nwww.wandb.ai  •  contact@wandb.ai\\nLeveling Up ML Capabilities In Your \\nOrganization\\nThe MLOps discussion is often focused exclusively on tools. However, one \\ncritical aspect of successful machine learning operations is enabling people to \\nsucceed, which involves designing the right structure for your organization.\\nAdditionally, you should consider the unique aspects of machine learning and how general software \\ndevelopment principles may not be applicable to those projects.\\nOrganization Design\\nOrganizational structure influences how well people are aligned towards achieving business goals. \\nDepending on the maturity of your organization, machine learning roles can be embedded in \\nbusiness teams instead of centralized in a separate organization. Some companies may also benefit \\nfrom a dedicated group that is focused on managing the machine learning platform.\\nDefining clear roles is fundamental to organizational design. Well-defined roles allow people to \\nfocus on areas of interest without being spread too thinly; the surface area of machine learning \\nsystems is huge and it’s often intractable for one person to cover all of the requirements effectively.\\nBelow are some concrete areas of proficiency, along with associated job titles:\\n• Data Scientist: Explore data and find insights to influence business and product decisions.\\n• Analytics Engineer: Define and productionize business metrics.\\n• Machine Learning Engineer: Develop and evaluate machine learning models.\\n• Machine Learning Researcher: Experiment with new ML architectures and methods.\\n• Data Platform Engineer: Manage the data infrastructure.\\n• MLOps Engineer: Manage machine learning platforms and operations.\\n• Machine Learning Auditor: Control risks associated with ML solutions.\\n• Product Manager: Define what machine learning solutions to build.\\n• Software Engineer: Integrate ML capabilities into existing applications.\\nThese slides from the Full Stack Deep Learning course provide an excellent overview of \\norganizational design considerations for machine learning teams.\\nManaging Machine Learning Projects\\nClassic project management approaches like waterfall, agile, scrum, etc. may not be the best \\nfit for machine learning. Often, model performance and characteristics are unknown or highly \\nspeculative, making it challenging to estimate the business impact or timelines of some machine \\nlearning projects up front.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 7}),\n",
       " Document(page_content='9\\nwww.wandb.ai  •  contact@wandb.ai\\nProgress on machine learning projects can plateau or have unexpected breakthroughs. The graph \\nbelow, from this blog post by Lukas Biewald (CEO of Weights & Biases), illustrates this concept well.\\nHere, we can see the best accuracy achieved in a Kaggle competition in the first week vs. the first \\nthree months. Results like this are common. Sometimes ML projects see massive leaps in a short \\nperiod of time and with little effort, while other times there is little progress despite significant \\neffort. It can be very difficult to know which is more likely.\\nUncertainty exists not only in how long specific tasks might take, but also in which tasks to perform \\nat all. Later tasks are often highly dependent on the outcome of earlier ones. For example, the early \\nefforts in a project are often dominated by experimentation and the results of those experiments \\ndictate which paths to pursue.\\nAccuracy improvement in first week\\nAccuracy improvement in three months\\nSource: Why are machine learning projects so hard to manage?', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 8}),\n",
       " Document(page_content='10\\nwww.wandb.ai  •  contact@wandb.ai\\nKnowledge Sharing\\nA key component in organizational effectiveness is ensuring that information and context \\nare shared, in order to drive alignment on company goals and minimize duplicate efforts. \\nThe importance of this becomes acute with data science and machine learning, since much of \\nthe work requires experimentation.\\nTools that help facilitate knowledge transfer and sharing of technical information, like W&B \\nReports, can uplevel the quality of data science and machine learning knowledge across \\norganizations. W&B Reports allow you to display and aggregate information seamlessly across \\nall of your W&B projects, without having to extract data into external systems. Reports also have \\na plethora of collaboration features, including commenting, shared editing, and the ability to be \\nembedded in a wide variety of platforms.\\nTools and platforms are important, but they are not sufficient to achieve success with machine \\nlearning projects alone. While some tools facilitate good processes, they cannot supplant them \\nentirely. Below, we’ll discuss the processes that are critical to successful ML projects.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 9}),\n",
       " Document(page_content='11\\nwww.wandb.ai  •  contact@wandb.ai\\nDesigning Good Processes  \\nAround ML Projects \\nScoping and Opportunity Sizing\\nAssessing which ML projects maximize business impact is not something that’s part of most data \\nscience training. Business impact can come in many forms: efficiency, fairness, accuracy, or—most \\nobviously—value. \\nThis assessment is an essential skill for career success as a data scientist, but is most commonly \\na skill learned on the job through repetition and experience. Business partners often assume that \\nmachine learning is the answer to their problems, even when it may not be. Conversely, they may \\nbe skeptical of machine learning. Picking the right projects and assessing business value upfront is \\ncritical for long-term success. \\nThankfully, there are concrete tools and approaches that can help teams assess the business \\nviability of a project before embarking on it. Below is a diagram that illustrates some of the things \\nyou may want to consider when assessing the business impact of a proposed ML project:\\nIf you’d like a bit more information, the “Assessing The Feasibility of The Project” section of our \\nblog post on optimizing business value with ML provides a concrete example of a structured \\napproach to assessing ML projects.A subset of possible business profit drivers that should be considered when sizing the opportunity of a potential machine learning \\nproject. Source: “Data Project Checklist” , fast.ai', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 10}),\n",
       " Document(page_content='12\\nwww.wandb.ai  •  contact@wandb.ai\\nData Discovery and Validation\\nWhen we first embark on an ML project, we must do a significant amount of exploration to identify \\nif relevant data exists and where it exists, as well as how to extract, transform, and validate it. \\nOften, the data will not already exist in an easy-to-retrieve SQL table. Furthermore, the veracity and \\nfreshness of the data may be unknown, and data engineers will be expected to validate and ensure \\ncontinued data quality.2\\nData quality insurance can take many forms including: \\n• Type-checking (no strings in your bool column) \\n• Distributional expectations (your average doesn’t suddenly shift by an order of magnitude)\\n• Value errors (your color picker doesn’t allow \"dog\")\\n• Diversity (your AV dataset has examples of firetrucks avoiding a redlight), and\\n• Consistent volume (today’s new user signups is not down 99%).\\nIt is also important that this early-stage discovery process—usually called Exploratory Data Analysis \\n(EDA)—is documented and reproducible. For example, this W&B Report demonstrates EDA for the \\nWorld Happiness Report. \\n2. Data Quality Fundamentals, Barr Moses, Lior Gavish, and Molly Vorweck, 2022.\\nUsing parallel coordinate plot in W&B for exploratory data analysis. Source: Planetary Well-Being Metrics', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 11}),\n",
       " Document(page_content='13\\nwww.wandb.ai  •  contact@wandb.ai\\nModel Development and Evaluation\\nBefore we operationalize models, we need to conduct experiments to validate their feasibility \\nand estimate their business impact. This often involves building a simple baseline, followed by \\ncollaborative and iterative experimentation and model evaluation. We’ll discuss each of  \\nthese stages below.\\nThe first step of a new machine learning project should be understanding the business context, which \\nwe discussed in \"Scoping and Opportunity Sizing\" . In parallel, we should engage in \"Data Discovery \\nand Validation\" to explore and understand the data. Finally, we need to define a relevant model \\nperformance metric that correlates with our business objective. It’s often beneficial to build a simple \\nbaseline and review it with subject matter experts before investing in advanced modeling.\\nFeedback from subject matter experts is critical in effective experimentation. Conducting error \\nanalysis with their input can help direct future investments. For example, we may decide to extend \\nour training data, change our model’s architecture, or improve our labeling process. It is important to \\nestablish tools that allow rapid iteration and collaboration with stakeholders.\\nWe often need to look at metrics not only in the aggregate, but also on specific slices of data. For \\nexample, with self-driving cars, segmenting all classes from a camera feed correctly is important, \\nbut we also need to be able to look at the metrics for specific classes such as pedestrians. This W&B \\nReport provides an example of error analysis on an autonomous vehicle task.\\nA robust model development and evaluation process will allow you to iterate fast, effectively \\ncommunicate with stakeholders, and properly evaluate models. Weights & Biases Experiments and \\nReports can facilitate these tasks.\\nDevelopment Tools\\nOne prerequisite of effective model development and evaluation is setting up a development \\nenvironment, which often involves configuring powerful remote compute environments rather \\nthan engineers’ laptops. The features listed below are needed to create repeatable and reliable \\ndevelopment environments.\\n• Dependency Management\\nMost machine learning models and pipelines are developed in Python and require \\ndependency management tools such as Poetry or Conda. Docker , virtualenv , or Pipenv can \\nalso simplify the process of developing and deploying machine learning models by ensuring a \\nconsistent environment.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 12}),\n",
       " Document(page_content='14\\nwww.wandb.ai  •  contact@wandb.ai\\nBecause there are such a wide variety of dependency and environment management \\ntools, it is beneficial to standardize on a few, since disparate systems across a team reduce \\ncollaboration and effectiveness. Additionally, creating paved paths for using dependency \\nmanagement across prototyping and production generally pays significant dividends down \\nthe line.\\nW&B offers excellent support for a wide variety of dependency management frameworks, including \\nDocker , conda, or pip .\\n• Sandbox Environments\\nMany companies use cloud providers or on-prem hardware for their compute needs. However, \\ndevelopers need the ability to create sandboxed environments with all of the tools that are \\nconsistent with your production environment. This may require creating a locked-down \\ncontrol plane to limit the surface area of what can be utilized. Minimizing the discrepancy \\nbetween the development and production environment is key for enabling rapid iteration.\\n• General Development Tools\\nThere are many tools that help ML practitioners be more productive while experimenting. \\nThese may include terminal tools (e.g. tmux), IDEs (e.g. VSCode, PyCharm, Jupyter Lab), code \\nversioning tools (e.g. git), and debugging tools3. It is important that your sandbox environment \\nsupports these general development tools, in order to enable a minimum viable developer \\nexperience for ML engineers.\\n• Jupyter Notebooks\\nJupyter Notebooks are central to a machine learning engineer’s ability to rapidly prototype \\nand experiment with ideas. However, there is much debate in the community about how to \\nuse notebooks effectively, with some even claiming that notebooks should not be used at all. \\nNevertheless, there are tools and processes that enable teams to use notebooks effectively \\nwhile still maintaining software engineering best practices, including distributing Python \\npackages, writing tests, and documentation. Tools like nbdev , Metaflow , and Quarto offer \\nfacilities for developing and deploying production-grade software and sharing knowledge with \\nnotebooks.\\nW&B also allows you to leverage the full power of W&B features from a notebook with its \\nexcellent set of notebook integrations.\\nDecision Optimization\\nML is often used to optimize operational decisions. However, models only provide predictions, not \\nbusiness decisions. We need to calibrate how to use our model to make decisions. For example, in \\nthe context of classification, if the cost of false positives is very high, we might tweak our decision \\nthreshold to prioritize fewer false positives at the cost of more false negatives, and ultimately \\ngreater business impact. Failure to calibrate4 decision thresholds may result in negative business \\nvalue, despite having predictive models.\\nThe “Tuning The Model For Business Value” section of this blog post provides a decision \\noptimization framework, demonstrated with a practical example that shows you how to maximize \\nbusiness value.\\n3. W&B provides optional code snapshotting to tie back versions of the code used to generate experiments logged to W&B. \\n4. \"Classifier Calibration\" , Bryan Bischof, 2022', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 13}),\n",
       " Document(page_content='15\\nwww.wandb.ai  •  contact@wandb.ai\\nW&B also facilitates error analysis of models.  For example, this Report illustrates where a computer \\nvision model struggles to discern amongst similar animal types, enabling an ML engineer to analyze \\nand improve on model errors.\\nGovernance\\nThere are many sources of bias that can occur in machine learning. It is important to understand \\nwhat each of these types of biases are, their root causes, and how to implement systems and \\nprocesses to mitigate them. Here is a diagram of seven common types of biases, as illustrated in \\nthe paper “ A Framework for Understanding Sources of Harm throughout the Machine Learning Life \\nCycle ”:\\nSeven types of biases introduced at various stages in the machine learning project lifecycle.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 14}),\n",
       " Document(page_content='16\\nwww.wandb.ai  •  contact@wandb.ai\\nLet’s quickly define the biases in the diagram above: \\n• Historical bias: How data is originally generated. \\n• Representation bias: How the data sampling strategy is defined.\\n• Measurement bias: How the data sampling strategy is carried out.\\n• Aggregation bias: How models are defined. \\n• Learning bias: How models are trained on sampled data. \\n• Evaluation bias: How models are validated against benchmarks.\\n• Deployment bias: How the final implementation of the machine learning system is \\nexperienced by the world.\\nW&B Reports can be leveraged in your overall governance process to identify biases in your data \\nand models, as well as to share that knowledge across the organization. For example, this W&B \\nReport outlines various tactics you can use to discover bias and fairness issues in your models. \\nFurthermore, W&B can assist you with data privacy management, like detecting issues such as \\ninadvertently training models on private data. W&B’s Artifact lineage provides visibility into all of \\nthe data used by your models and metrics, which can be used to satisfy compliance audits related \\nto data access. For example this Report shows examples around regulatory audits.\\nData Engineering\\nA prerequisite for training machine learning models is data preparation. This makes data \\nengineering a central component of any ML operationalization task. \\nIn \"Data Discovery and Validation\" , we discussed how to most effectively discover, validate, and \\nprepare data. However, this data transformation and validation needs to be repeatable and \\nconsistent in order to operationalize models.\\nData pipelines allow us to automate the sourcing, transformation, and validation of data for \\nmodel training and inference. A feature store can also help with consistency and reproducibility by \\norganizing your features in a transparent way. Feature stores can also provide features on demand \\nfor high-availability and low-latency environments.\\nMost feature stores also ensure that your batch data and your streaming data stay in sync. \\nFor example, Linkedin built Feathr  to promote consistency of data across the training and \\nserving of models:', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 15}),\n",
       " Document(page_content=\"17\\nwww.wandb.ai  •  contact@wandb.ai\\nFurthermore, if your dataset requires frequent labeling or human review, it may be helpful to use \\nexisting models for assistance. For example, you can use a model's feedback to prepare labels—to \\nbe verified later by an expert annotator—and select the most valuable examples for \\nlabeling (active learning).\\nEven though data engineers may shoulder some of this orchestration and pipelining work, we find \\nthat having an understanding of data engineering techniques is critical for ML practitioners to be \\neffective when designing an ML product. W&B Artifacts can help you couple data engineering tools \\nwith your model pipelines by snapshotting and tracking data lineage associated with models.\\nFor example, W&B Artifacts will build a dependency graph of data for you as data flows through \\nyour machine learning pipeline, as well as allow you to version and track how your datasets change \\nover time. Artifacts also link registered models to their associated data artifact:\\nThe purpose of Feathr, and feature stores in general, is to unify data processing for both “offline” processes, such as model training, and \\n“online” processes, such as model inference. Source: Linkedin Engineering Blog\\nW&B Artifacts let you track the dependencies of your data across multiple versions.\", metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 16}),\n",
       " Document(page_content=\"18\\nwww.wandb.ai  •  contact@wandb.ai\\nScaling, Orchestration, and Reproducibility\\nAfter developing a baseline, we often need to prepare our model for production along \\nthe following dimensions:\\n• Scale\\nYou may need to train additional (potentially many more) models for specific customers or \\nproducts. Often, you may also want to increase your model’s capacity so that it can learn \\nfrom a larger dataset. You may also need to compress your model so that it can meet your \\norganization's constraints (latency, memory, etc.). \\nBut how does a machine learning engineer or data platform engineer scale their systems to \\ntrillions of inferences per day? Some canonical methods borrowed from software engineering \\ninclude horizontal and vertical scaling, load balancing, localization, replication, and \\nasynchrony.\\nWhile these software engineering methods are important and valuable, MLOps has some \\nadditional considerations with respect to scale. Two important data processing regimes in ML \\nare batch  and streaming.\\nFor applications that utilize data that is known ahead of time, pre-processing that data as a \\nbatch can make the application much more performant. Data that is not knowable ahead of \\ntime may require a streaming approach. A combination of batch and streaming approaches \\nare often necessary, which requires a system for processing data from both regimes in a \\nconsistent and performant way. \\n• Orchestration\\nIn order to ensure that your models do not become stale, you may want to regularly retrain \\nthem on recent data. You may also want to automate model testing and evaluation to \\nensure that retraining actually improves model performance. Lastly, you should ensure that \\ndownstream or upstream dependencies are refreshed appropriately when your model is \\nretrained. \\nGenerally, creating these workflows starts by expressing your pipeline as a directed acyclic \\ngraph (DAG). These pipelines need to be integrated with compute resources and be able to \\ntrack artifacts and metrics associated with each execution. The orchestration engine (e.g. \\nDagster , Airflow , Prefect ) takes care of executing the pipeline based on a schedule or a specific \\ntrigger (for example, when new data arrives). \\n• Reproducibility\\nPeople on a team should be able to debug and collaborate on machine learning workflows \\nso that they can maintain them. Workflows should be reproducible, which involves not only \\nsnapshotting code but also data and any user inputs for running the model.\\nAnother important aspect of reproducibility is portability. Portability is achieved when you \\ncan easily integrate components of your workflow into other workflows and quickly add new \\ncomponents in a modular way.\\nW&B Launch provides tools that can get you started with scaling, orchestrating, and \\nreproducing experiments. You can use it to execute runs in reproducible containerized \\nenvironments as well as queue and launch jobs across your own local or cloud clusters.\", metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 17}),\n",
       " Document(page_content='19\\nwww.wandb.ai  •  contact@wandb.ai\\nCI/CD and Testing For Machine Learning\\nIn traditional software development, continuous integration/continuous deployment (CI/CD) is \\nused to automate many tasks including testing, building, and deploying software. CI/CD can also be \\nadapted to ML projects. For example, in addition to unit tests for your code, you can perform smoke \\ntests on your models (i.e. validating that your models can predict and learn from example data). You \\ncan also have CI/CD systems retrieve data from W&B and display relative metrics in a pull request.\\nIt is important that you establish some level of testing and automation when you change code \\nassociated with your ML workflows. Using CI/CD tools where available will also help you build trust \\nwith your engineering peers by being deliberate about software tests and making those results \\nvisible to everyone on your team.\\nBelow is a demo that uses Kubeflow, W&B, and GitHub Actions to demonstrate CI/CD for ML: \\nMachine learning operations with GitHub Actions and Kubernetes - GitHub Universe 2019\\nDeployment and Observability\\nIn order to realize business value from our machine learning models, we need to actually deploy \\nthem. When designing a model deployment system you will have to consider latency (batch, real-\\ntime, streaming), scale, and security requirements. \\nGenerally speaking, batch predictions are simpler than real-time or streaming because you can \\nasynchronously write raw data and predictions to a database without having to worry too much \\nabout latency, memory constraints, or web servers being able to handle prediction requests.\\nWhen running inference at scale, it may be important to optimize the model to reduce latency and \\ninfrastructure costs. Traditional models, such as decision trees, can be compiled (optimized for \\ninference execution) to improve efficiency. For deep learning models, we can perform distillation \\nor use lower floating point precision to reduce their size, as well as use frameworks to serialize and \\noptimize models to run on specific hardware5.\\nMoving a model to production requires following a defined workflow. For example, we often want \\nto define criteria to promote a model from development to staging, and from staging to production. \\nAdditionally, we need to be able to track model lineage so that we can associate models with data \\nand evaluation metrics at various parts of the workflow. This type of observability can be enabled \\nthrough W&B Models.\\nAnother kind of observability is monitoring our model’s performance in real time. We will want to \\nmonitor the distribution of input data to detect and react to data drift.  Furthermore, it often makes \\nsense to have a human in the loop looking at a fraction of model predictions and verifying model \\nperformance, especially on new information or edge cases. New, freshly labeled data can then be \\nfed into a live training loop to improve model performance6. \\nFinally, we want to make sure we are monitoring business metrics associated with ML projects. \\nIf we deploy an improved ML model, we want to verify that it improves our business metrics. \\n5. One such example is model quantization.\\n6. This is often referred to as \"continuous learning\" .', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 18}),\n",
       " Document(page_content='20\\nwww.wandb.ai  •  contact@wandb.ai\\nBefore deploying your models, it can be useful to conduct A/B tests to verify that your new model is \\nbetter than an incumbent model. Utilizing randomization, we can build statistical confidence that \\nobserved differences in metrics between two subpopulations are due to the variation between their \\nexposure to one model or the other. Experimentation can also come in the form of reinforcement \\nlearning, but the A/B testing framework is the most common approach to evaluate the effect \\nof a new model.\\nFor example, Airbnb leveraged A/B testing tools (pictured below) to ensure that recommendation \\nmodels were affecting business metrics:\\nTo enable A/B tests and observability of your deployment workflows, it is necessary to version and \\ntrack your model deployment candidates. A model registry can help you track and version models \\nfrom staging to production, as well as give you visibility into a model’s history. \\nW&B Models provide you with robust tools to version, track, and organize your models for the \\npurposes of deployment.“We conducted an A/B test to compare the new setup with  \\n2 models with Personalization features to the model from Stage 1.  \\nThe results showed that Personalization matters as we were  \\nable to improve bookings by +7.9%…”\\nSource: Machine Learning-Powered Search Ranking of Airbnb Experiences\\nAirbnb leverages the powerful A/B testing platform pictured here to make decisions that improve customer satisfaction. \\nSource: Machine Learning-Powered Search Ranking of Airbnb Experiences', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 19}),\n",
       " Document(page_content='21\\nwww.wandb.ai  •  contact@wandb.ai\\nML Platforms \\nIt is often hard to decouple processes from technology. In an ideal world, we \\nwould start with the process and pick the right technology to support it. In \\nreality, technology often influences or constrains our processes. \\nIt is therefore important to build a platform that is flexible enough to support your workflows, while \\nalso integrating well with other solutions in your stack. We have highlighted many tools throughout \\nthis article that you can use to compose an end-to-end ML Platform, which we’ll summarize \\nlater in this section. \\nFrom a technological perspective, there are three main categories of tools relevant to ML:\\n• Frameworks\\nWhile low-code and no-code platforms are gaining some traction, most ML practitioners prefer \\ncode to train and evaluate models. There are many frameworks (often open-source) at various \\nlevels of abstraction that support this, including TensorFlow , Keras , PyTorch, Lightning, fastai , \\nXGBoost , and JAX. Similarly, there are frameworks focused on particular ML tasks like \\nYOLOv5, Detectron, and GPT-3.\\n• Ops tools\\nPerforming every step of the ML workflow manually is not efficient. Adopting developer tools \\ncan make ML practitioners more productive, promote best practices, and improve business \\nresults. Some popular tools in this category help with tasks like experiment tracking, handling \\nfeatures, labeling, versioning, serving, monitoring, and orchestration.\\n• Compute and Storage\\nIn order to execute ML workflows, we need storage and compute provided through the \\ninfrastructure layer. Both are most often sourced through cloud computing (AWS, GCP , Azure) \\nbut some companies choose to manage this on their own (on premises).\\nFor reasons discussed below, we believe good ML platforms offer maximal flexibility with regards \\nto ML frameworks, leaving the data scientists free to work with any framework they choose. \\nFurthermore, your compute and storage decisions are likely solidified well before you start to \\nengage in building an ML Platform. Therefore, this section will focus primarily on Ops tools.\\nConsiderations When Selecting Tools\\nSome important considerations when building an ML Platform are: skill sets, abstractions, existing \\ninfrastructure, monoliths vs. point solutions, and build vs. buy. We’ll discuss some of these below.\\nSkill Sets\\nWhen deciding which tools to use as part of your platform, it is important to select tools in the \\ncontext of your organization’s skill set. For example, if a tool assumes knowledge of Kubernetes, \\ndoes someone with sufficient Kubernetes experience work on your team?', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 20}),\n",
       " Document(page_content='22\\nwww.wandb.ai  •  contact@wandb.ai\\nIf not, does it make sense to hire a person if this would be the only use of their skills? You should \\nalso make sure to understand if there might be people on your team who are already familiar with \\ncertain tools from previous jobs or roles that you can leverage right away. \\nAnother consideration that is often overlooked is how your team wants to grow professionally in \\ntheir careers. Engineers on your team with a growth mindset may view the opportunity to use new \\ntools as a strong incentive to keep working at your company. Similarly, prospective new hires often \\nscreen companies based on the tools they use. You must balance leveraging the skills you already \\nhave vs. learning new tools.\\nAbstractions\\nIt is important to build the right abstractions to help your team focus on high-value activities. \\nThis can be a tricky balance because there is a fundamental mismatch between how much \\ninfrastructure is needed at different parts of the stack and what data scientists care about:\\nThe more data scientists care about a particular area, the more flexibility they will want. As a rule, it \\nis often a good idea to create an ML Platform that gives data scientists a wide range of freedom on \\nmodel development and feature engineering tools, while giving them high-level abstractions that \\nallow them to reduce time spent configuring compute resources, even if that limits their flexibility \\nin those areas.\\nFinally, it is important to try to meet your developers where they are. For example, for data \\nscientists, this can mean offering them familiar APIs such as Python or SQL, and supporting \\ndevelopment environments they’re comfortable with, like Jupyter Notebooks.Data scientists tend to care less about capabilities that require more technological infrastructure. This hypothesis underpins much of the \\nMLOps discussion today. Source: Effective Data Science Infrastructure', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 21}),\n",
       " Document(page_content='23\\nwww.wandb.ai  •  contact@wandb.ai\\nExisting Infrastructure\\nYou may have existing data or other infrastructure prior to building an ML Platform. If so, it is \\nuseful to determine whether the set of tools you are considering complements your existing \\ninfrastructure. In other words, if you are using Kubernetes, it might be desirable for you to deploy \\nnewly adopted tools on Kubernetes in order to leverage paved paths that already exist in terms of \\nskill sets, security, or observability. Additionally, if your pipeline is not easily schedulable, you may \\nneed to create custom integrations with your existing platform.\\nPoint Solutions vs. Monoliths\\nPoint solutions tackle one area of the ML workflow really well. Because point solutions are focused, \\nthey can be feature-rich for a specific task. On the other hand, monoliths provide a broad range of \\ninfrastructure in one package but may not focus very deeply in any one area.\\nIt is important to note that most tools are not strictly point solutions or monoliths but exist on a \\nspectrum. Also, point solutions often drift towards monoliths over time as more features are added. \\nWhen evaluating tools in this context, it is important to pay attention to this dynamic and assess \\nthe features you care about most.\\nLastly, point solutions also tend to offer more flexibility in terms of interoperating with other tools \\nand are easier to onboard, whereas monoliths offer a more opinionated set of tools that you do not \\nhave to glue together yourself. You should consider these tradeoffs when selecting tools for your ML \\nPlatform.\\nBuild vs. Buy\\nWhen designing your ML platform, you need to decide between in-house development and buying \\nthe ML tools. Here are some factors you should consider in your decision-making process. \\n• Business value of ML projects\\nYou’ll want to maximize the business value of your ML projects, and having tools that support \\nyour data scientists to get better outputs faster will increase the top line. You’ll get better \\noutputs by using tools that are familiar to your engineers and offer great user experience. \\n• Total cost of ownership\\nHiring engineers to develop your own tools can be expensive. IT projects also often overrun \\ninitial estimates, which you should factor into your budget if pursuing in-house development.\\n• Maintenance and scalability\\nThere are costs associated with maintaining and monitoring in-house solutions. You may get \\ndowntimes that also impact your business value. As your solution scales, the maintenance \\nrequirements often grow exponentially.\\nOther Considerations\\nThere are many other considerations such as cost, support, and maturity. However, in our \\nexperience, the categories we explored above are particularly important—yet often overlooked—\\nwith respect to machine learning.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 22}),\n",
       " Document(page_content='24\\nwww.wandb.ai  •  contact@wandb.ai\\nHow Weights & Biases Fits In\\nWeights & Biases offers a machine learning platform for developers with a focus on improving \\nobservability and iteration speed. Using W&B is extremely lightweight, as you can get started with \\njust a few lines of code. W&B allows you to track experiments, version and iterate on datasets, \\nevaluate model performance, reproduce models, visualize results, identify errors, and share \\nfindings with colleagues. \\nThe platform consists of the following features:\\n• Experiments\\nExperiments enable you to track and visualize experiments in real time, compare baselines, \\nand iterate quickly on ML projects. The W&B Python library is lightweight, compatible \\nwith workflows of any shape and size, and can be easily integrated into your training and \\nevaluation pipelines.\\n• Reports\\nReports let you organize and embed visualizations, describe your findings, and share updates \\nwith collaborators. As a tool for project management and collaboration in machine learning \\nprojects, Reports combine the power of interactive, dynamic visualizations with text and rich \\nmedia.\\n• Artifacts  \\nArtifacts make it easy to get a complete and auditable history of changes to your files by \\nenabling dataset versioning, model versioning, and the tracking of dependencies and results \\nacross machine learning pipelines. Store entire datasets directly in Artifacts, or use Artifact \\nreferences to point to data in other systems like S3, GCP , or your own bucket.\\n• Tables\\nTables allow you to log, query, and analyze tabular data in order to understand higher-level \\npatterns in your data. Compare changes precisely across models, epochs, or individual \\nexamples, understand datasets, visualize model predictions, and share insights in a central \\ndashboard.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 23}),\n",
       " Document(page_content='25\\nwww.wandb.ai  •  contact@wandb.ai\\n• Sweeps\\nSweeps help automate hyperparameter optimization and explore the space of possible \\nmodels. Optimize models more quickly and free up time for teams to focus on other parts of \\nthe machine learning pipeline.\\n• Models\\nModels provide a central system of record for models. Create registered models to organize \\nyour best model versions for a given task, and track models as they move into staging and \\nproduction.\\nW&B doesn’t address the entire ML workflow, but rather focuses on providing a set of best-in-class \\ntools for several areas. This is why W&B offers integrations with a wide variety of data science \\ninfrastructure, such as Databricks, Metaflow , Kubeflow , Prodigy , Ray , SageMaker , Lightning, \\nMosaicML, and more. This flexibility enables you to compose an end-to-end ML Platform that fits \\nyour needs.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 24}),\n",
       " Document(page_content='26\\nwww.wandb.ai  •  contact@wandb.ai\\nAddressing MLOps Opportunities With \\nW&B \\nAs discussed throughout this piece, effective MLOps is characterized \\nby putting the right processes in place and organizing the right people \\nappropriately. \\nRevisiting our original checklist, here are some of the ways W&B can help you build a platform \\nthat’s right for your unique organization:\\n`\\nOrganization Design Knowledge Sharing Project ManagementPeople\\nW&B Reports facilitate \\ncollaboration and \\ncommunication \\nacross a wide variety \\nof disciplines in \\nyour organization. \\nThis allows people \\nin various roles in \\nyour organization to \\nparticipate in an end-\\nto-end workflow.W&B Reports facilitate \\nknowledge sharing \\nwith features like \\ncommenting, shared \\nediting, and the ability \\nto be embedded \\nin a wide variety of \\nplatforms.\\nYou can use W&B \\nwith other tools that \\nfacilitate knowledge \\nsharing like nbdev \\nand Quarto .W&B Experiments \\nand Reports give \\nproject managers \\nunparalleled visibility \\ninto the status of \\nmachine learning \\nprojects, while \\ncomplementing their \\nexisting tools \\nand processes.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 25}),\n",
       " Document(page_content='27\\nwww.wandb.ai  •  contact@wandb.ai\\nScoping and  \\nOpportunity SizingModel Development  \\nand EvaluationData Discovery  \\nand ValidationProcesses\\nW&B Reports allow \\nyou to share analysis \\nand presentations, \\nincluding those \\nrelated to scoping \\nand opportunity \\nvalidation.W&B Experiments and \\nReports help facilitate \\nmodel development \\nand evaluation.\\nW&B Sweeps automate \\nhyperparameter \\ntuning, improving the \\nproductivity of both \\nyour engineers and \\nyour GPUs.\\nW&B Tables allow you \\nto evaluate model \\npredictions in an \\ninteractive and \\nvisual way.\\nFurthermore, W&B \\noffers integrations \\nwith popular ML \\nframeworks that \\nare focused on \\ndevelopment and \\nevaluation such as: \\nscikit-learn, Keras , \\nPyTorch, fastai , \\nXGBoost , TensorFlow , \\nSpaCy , and many \\nmore.W&B Tables allow \\nyou to explore and \\nvisualize your data, \\nand can be used \\nalongside other tools \\nin Jupyter Notebooks.\\nW&B Reports allow \\nyou to document and \\nshare findings from \\ndata discovery and \\nvalidation with your \\nteam, as illustrated in \\nthis example.Decision Optimization\\nData EngineeringGovernanceW&B Tables allow you \\nto perform analyses \\non your model’s \\nmetadata including \\nmodel calibration and \\ndecision optimization. \\nFurthermore, these \\ntools can be used in \\nJupyter Notebooks for \\nadditional flexibility \\nand integration with \\nother tools.\\nW&B Artifacts can be \\nused to couple data \\nengineering tools with \\nyour model pipelines \\nby snapshotting and \\ntracking data \\nlineage associated \\nwith models.W&B Reports can \\nbe used to augment \\nyour governance \\nprocess as illustrated \\nin this example, which \\nincludes various \\ntactics you can use \\nto investigate model \\nfairness.\\nThe W&B API can be \\nused to integrate with \\ngovernance-related \\ntools such as fiddler.ai.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 26}),\n",
       " Document(page_content='28\\nwww.wandb.ai  •  contact@wandb.ai\\nScaling, Orchestration,  \\nand ReproducibilityDeployment and \\nObservabilityCI/CD and Testing For  \\nMachine Learning\\nW&B Launch provides \\ntools that can get \\nyou started with \\nscaling, orchestrating, \\nand reproducing \\nexperiments.\\nFurthermore, W&B \\nintegrates with \\nrelevant tools such as \\nMetaflow , Kubeflow , \\nSageMaker , \\nand Databricks.W&B Models provide \\nyou with robust tools \\nto version, track, and \\norganize your models \\nfor deployment \\npurposes.\\nWith the W&B API, you \\ncan retrieve models \\nfrom the registry \\nfor A/B testing and \\nserving with the tools \\nof your choice. The W&B API and \\nCI/CD systems like \\nGitHub Actions can \\nbe used to enable \\nworkflows like those \\ndemonstrated here:\\nMachine learning \\noperations with \\nGitHub Actions and \\nKubernetes - GitHub \\nUniverse 2019 \\nCost and Time to Value Fit for Purpose Maintainability,  \\nInteroperability, and \\nReliabilityPlatform\\nW&B’s user-based \\npricing model is \\neasy to calculate and \\nbudget.\\nW&B is easy to set \\nup and start using \\nquickly, while also \\nbeing capable of \\nrunning thousands \\nof experiments at \\nscale to build models \\ncollaboratively. \\nW&B tracks hardware \\nusage for machine \\nlearning experiments, \\nwhich allows for \\noptimization while \\navoiding wasting \\nexpensive resources.W&B keeps investing \\nin a platform that \\nis co-designed with \\ntop-notch institutions \\nsuch as OpenAI and \\nused by hundreds of \\ncompanies around the \\nworld. \\nOver 300,000 ML \\npractitioners love \\nusing W&B.W&B performs \\nmonthly high-quality \\nreleases while \\nconstantly improving \\nthe platform, ensuring \\nan uptime greater \\nthan 99.95% and a \\nhigh NPS over 75. \\nW&B is integrated \\ninto every popular \\nML framework and \\ninstrumented in \\nthousands of popular \\nML repos.\\nTechnology partner \\nintegrations are a \\nsmooth experience \\nwith W&B and \\nAmazon Sagemaker, \\nVertex AI, Kubeflow, \\netc.', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 27}),\n",
       " Document(page_content='29\\nwww.wandb.ai  •  contact@wandb.ai\\nGet In Touch\\nOperating machine learning systems \\nrequires a comprehensive design \\nthat consists of people, processes, \\nand platforms. We believe this guide \\nprovides a holistic overview of the \\ncomponents of highly functional \\nmachine learning operations.\\nIf you would like to learn more about \\nW&B and how we can partner with \\nyour organization to drive business \\nvalue with ML, sign up or request a \\ndemo . • This in-depth survey describes \\nchallenges and opportunities of \\noperationalizing ML. The survey \\nwas conducted across a broad \\nrange of industries and domains.\\n• Full Stack Deep Learning is a \\ncourse that dives deeper into \\nsome of the topics discussed \\nin this article, such as \\nconsiderations when building ML \\ninfrastructure as well as how to \\norganize ML teams. \\n• Designing Machine Learning \\nSystems by Chip Huyen goes into \\ngreat depth regarding the various \\ncomponents of ML systems and \\nassociated best practices.\\nwww.wandb.ai  •  contact@wandb.ai\\nDarek Kłeczek is a Machine Learning Engineer at Weights & Biases, where he \\nleads the W&B education program. Previously, he applied machine learning \\nacross  supply chain, manufacturing, legal, and commercial use cases. He also \\nworked on operationalizing machine learning at P&G. Darek contributed the first \\nPolish versions of BERT and GPT language models and is a leader in the Polish \\nNLP community. He’s a Kaggle competition winner and 3x Kaggle Master. \\nDr. Bryan Bischof is an adjunct professor of Data Science at Rutgers University, \\nand formerly the Head of Data Science at Weights & Biases. He’s worked in time \\nseries signal processing at scale, demand forecasting, global optimization and \\nlogistics, and personalized recommendations at companies like Stitch Fix, Blue \\nBottle Coffee, and IBM. He’s obsessed with math, and has a dog named Ravioli. \\nYou can find him on Twitter (@BEBischof ) and LinkedIn. \\nHamel Husain is currently an entrepreneur in residence at fast.ai, where \\nhe builds tools for data scientists and machine learning engineers. Hamel \\nhas previously worked at Airbnb , DataRobot , and GitHub where he built \\na wide array of machine learning products and infrastructure. Hamel \\nhas contributed to data and infrastructure tools in open source such \\nas Metaflow , Kubeflow , Jupyter , and Great Expectations. Hamel was also \\na consultant for over 10 years, and used data science to improve business \\noutcomes in the restaurant, entertainment, telecommunications, and retail \\nindustries. You can learn more about Hamel on his website.\\nFurther Reading', metadata={'source': './local-model/MLOPS whitepaper.pdf', 'page': 28})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./local-model/MLOPS whitepaper.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer the question based on the context below. If you can\\'t answer the question, reply \"I don\\'t know\".\\n\\nContext : Here is some conntext\\n\\nQuestion: Here is a question\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context : {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Here is some conntext\", question=\"Here is a question\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Asif.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"context\" : \"The name I was given was Asif\",\n",
    "        \"question\" : \"What's my name\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.form_document(\n",
    "    pages,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
